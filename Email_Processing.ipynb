{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### converting mbox files to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import mailbox\n",
    "import pandas as pd\n",
    "import re\n",
    "# Path to your .mbox file\n",
    "mbox_path = '../Mbox Files/Amine_All_mails.mbox'\n",
    "\n",
    "# Path where you want to save the CSV file\n",
    "csv_path = '../Output CSV/Raw Email/amine_emails.csv'\n",
    "\n",
    "# Define the email fields you want to extract\n",
    "fields = ['date_parsed', 'from', 'to', 'subject', 'body']\n",
    "\n",
    "# Function to extract the body of an email\n",
    "def get_email_body(message):\n",
    "    body = None\n",
    "    if message.is_multipart():\n",
    "        for part in message.walk():\n",
    "            if part.get_content_type() == 'text/plain':\n",
    "                body = part.get_payload(decode=True)  # decode\n",
    "                break\n",
    "    else:\n",
    "        body = message.get_payload(decode=True)\n",
    "    return body.decode('utf-8', errors='ignore') if body else None\n",
    "\n",
    "# Open the .mbox file\n",
    "mbox = mailbox.mbox(mbox_path)\n",
    "\n",
    "# Initialize a list to hold all email data\n",
    "emails = []\n",
    "\n",
    "# Iterate through each message in the .mbox file\n",
    "for message in mbox:\n",
    "    email_data = {\n",
    "        'date': message['date'],\n",
    "        'from': message['from'],\n",
    "        'to': message.get('to', 'N/A'),  # Handle cases where 'to' might be missing\n",
    "        'subject': message['subject'],\n",
    "        'body': get_email_body(message)\n",
    "    }\n",
    "    emails.append(email_data)\n",
    "\n",
    "# Convert the list of emails to a pandas DataFrame\n",
    "df_emails = pd.DataFrame(emails)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_emails.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(\"Conversion completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Engineering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from dateutil.parser import parse    \n",
    "\n",
    "def load_data(filepath):\n",
    "    \"\"\"Load data from a CSV file.\"\"\"\n",
    "    return pd.read_csv(filepath)\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Preprocess data by parsing dates and setting appropriate data types.\"\"\"\n",
    "    # Apply the function to the date column\n",
    "    df['str_body'] = df['body'].astype(str).replace({'None': '', 'nan': '', 'NaT': ''})\n",
    "    df['date_parsed'] = df['date'].apply(clean_and_parse_email_date)\n",
    "    df['date_parsed_pd'] = pd.to_datetime(df['date_parsed'],utc=True)\n",
    "    df['date_year'] = df['date_parsed_pd'].dt.year\n",
    "    df['date_year'] = df['date_year'].fillna(0).astype(int).astype(str)  # Fill NaN, convert to int, then to string\n",
    "    df['date_year'] = df['date_year'].replace('0', '')\n",
    "    df['date_quarter'] = df['date_parsed_pd'].dt.quarter.fillna(0).astype(int).astype(str).apply(lambda x: 'Q' + x)\n",
    "    df['date_YYYY_QQ'] = df['date_year'] + '-' + df['date_quarter']\n",
    "    df['date_YYYY_MM'] = df['date_parsed_pd'].dt.strftime('%Y-%m')\n",
    "    df['date_month_nb'] = df['date_parsed_pd'].dt.month\n",
    "    df['date_month_name'] = df['date_parsed_pd'].dt.month_name()    \n",
    "    df['date_day'] = df['date_parsed_pd'].dt.day\n",
    "    df['day_of_week_name'] = df['date_parsed_pd'].dt.day_name()\n",
    "    df['isweekend'] = df['date_parsed_pd'].dt.dayofweek >= 5\n",
    "    df['date_hour'] = df['date_parsed_pd'].dt.strftime('%Y-%m-%d %H:00')\n",
    "    df['time_of_day'] = df['date_parsed_pd'].dt.hour.apply(categorize_time)\n",
    "    df[['from', 'to', 'subject', 'body']] = df[['from', 'to', 'subject', 'body']].astype('string')\n",
    "    df['from'] = df['from'].fillna('Unknown Sender')\n",
    "    df['to'] = df['to'].fillna('Unknown Recipient')\n",
    "    df['clean_from'] = df['from'].apply(extract_email)\n",
    "    df['clean_to'] = df['to'].apply(extract_email)\n",
    "\n",
    "    return df\n",
    "\n",
    "def clean_and_parse_email_date(date_str):\n",
    "    \"\"\"Removing potential trailing descriptors like \"(GMT) and converting to timestemps\"\"\"\n",
    "    cleaned_date = date_str.split('(')[0].strip()\n",
    "    try:\n",
    "        # Parsing the cleaned date string into a datetime object\n",
    "        return pd.to_datetime(cleaned_date, format='%a, %d %b %Y %H:%M:%S %z', errors='coerce')\n",
    "    except ValueError:\n",
    "        # Fallback to fuzzy parsing if the strict format fails\n",
    "        return parse(cleaned_date, fuzzy=True, ignoretz=True)\n",
    "\n",
    "def categorize_time(hour):\n",
    "    if 0 <= hour <= 8:  # Night category now includes early morning\n",
    "        return 'Night'\n",
    "    elif 9 <= hour <= 17:  # Work-day during typical business hours\n",
    "        return 'Work-day'\n",
    "    elif 18 <= hour <= 23:  # Evening hours\n",
    "        return 'Evening'\n",
    "\n",
    "def extract_email(email):\n",
    "    \"\"\"Extract the email address using regex, return in lowercase.\"\"\"\n",
    "    match = re.search(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', email)\n",
    "    return match.group(0).lower() if match else email.lower()\n",
    "\n",
    "def filter_emails_that_contain_sku_in_title(df, keyword='EV'):\n",
    "    \"\"\"Filter emails by subject containing a specific keyword.\"\"\"\n",
    "    return df[df['subject'].str.contains(keyword, case=False, na=False)]\n",
    "\n",
    "def features_creation(df):\n",
    "    \"\"\"Orchestrate the creation of features by calling specific functions.\"\"\"\n",
    "    df = extract_sku_and_connections(df)\n",
    "    df = compute_rankings(df)\n",
    "    df = calculate_date_differences(df)\n",
    "    \n",
    "    df['constant_value'] = 1\n",
    "    df['row_id'] = df.index + 1  # Using +1 to make it 1-based index\n",
    "    return df\n",
    "\n",
    "def add_company_names(df, company_filepath):\n",
    "    \"\"\"\n",
    "    Enhance the DataFrame with company names associated with email addresses.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The main DataFrame containing 'clean_to' and 'clean_from' columns.\n",
    "    - company_filepath (str): The file path to the CSV containing company names and email associations.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The enhanced DataFrame with 'company_name_to' and 'company_name_from' added.\n",
    "    \"\"\"\n",
    "    # Read the extra data containing company names associated with emails\n",
    "    company_name_and_email_df = pd.read_csv(company_filepath)\n",
    "\n",
    "    # Merge to add 'company_name_from'\n",
    "    df = pd.merge(df, company_name_and_email_df[['email', 'Company Name']], left_on='clean_from', right_on='email', how='left')\n",
    "    df.rename(columns={'Company Name': 'company_name_from'}, inplace=True)\n",
    "    df.drop(columns=['email'], inplace=True)  # Remove the redundant 'email' column after merge\n",
    "\n",
    "    # Merge to add 'company_name_to'\n",
    "    df = pd.merge(df, company_name_and_email_df[['email', 'Company Name']], left_on='clean_to', right_on='email', how='left')\n",
    "    df.rename(columns={'Company Name': 'company_name_to'}, inplace=True)\n",
    "    df.drop(columns=['email'], inplace=True)  # Remove the redundant 'email' column after merge\n",
    "    \n",
    "    df[['company_name_from', 'company_name_to']] = df[['company_name_from', 'company_name_to']].astype('string')\n",
    "\n",
    "    return df\n",
    "\n",
    "def extract_sku_and_connections(df):\n",
    "    \"\"\"Extract SKU value and normalize email connections.\"\"\"\n",
    "    df['SKU'] = df['subject'].str.extract('(EV-[A-Z0-9-]+)')\n",
    "    df.loc[:, 'email_connection'] = df.apply(lambda x: format_connection(x['clean_from'], x['clean_to']), axis=1)\n",
    "    df.loc[:, 'company_connection'] = df.apply(lambda x: format_connection(x['company_name_to'], x['company_name_from']), axis=1)\n",
    "    return df\n",
    "\n",
    "def format_connection(from_field, to_field):\n",
    "    \"\"\"Format connections by sorting and normalizing case.\"\"\"\n",
    "    connected_field = sorted([from_field, to_field])\n",
    "    return ' - '.join(connected_field)\n",
    "\n",
    "def compute_rankings(df):\n",
    "    \"\"\"Compute various rankings.\"\"\"\n",
    "    df['email_rank_by_sku_pair'] = df.groupby(['SKU', 'company_connection'])['date_parsed_pd'].rank(method='first')\n",
    "    df['email_rank_by_sku'] = df.groupby(['SKU', 'company_connection'])['email_rank_by_sku_pair'].transform('max')\n",
    "    df['max_rank_by_sku_pair'] = df.groupby('SKU')['date_parsed_pd'].rank(method='first')\n",
    "    df['is_max_rank_equal_to_sku_pair_rank'] = df['email_rank_by_sku_pair'] == df['email_rank_by_sku']\n",
    "    return df\n",
    "\n",
    "def calculate_date_differences(df):\n",
    "    \"\"\"Calculate the date of the first email and the number of days from the first email for each group.\"\"\"\n",
    "    df['first_email_pair_date'] = df.groupby(['SKU', 'company_connection'])['date_parsed_pd'].transform('min')\n",
    "    df['days_from_first_email'] = (df['date_parsed_pd'] - df['first_email_pair_date']).dt.days\n",
    "    return df\n",
    "\n",
    "def is_internal_communication(connection):\n",
    "    \"\"\"\n",
    "    Checks if the communication is internal based on exact domain matching for 'bus.com'.\n",
    "\n",
    "    Args:\n",
    "    connection (str): A string containing two email addresses separated by ' - '.\n",
    "\n",
    "    Returns:\n",
    "    int: Returns 1 if both emails are from 'bus.com', 0 otherwise.\n",
    "    \"\"\"\n",
    "    emails = connection.split(' - ')\n",
    "    # Regex to match exactly 'bus.com' as the domain part of the email\n",
    "    pattern = r'[\\w\\.-]+@bus\\.com\\b'\n",
    "    \n",
    "    # Check both emails against the pattern\n",
    "    is_first_internal = re.fullmatch(pattern, emails[0].strip())\n",
    "    is_second_internal = re.fullmatch(pattern, emails[1].strip()) if len(emails) > 1 else False\n",
    "\n",
    "    # Return 1 if both are internal, 0 otherwise\n",
    "    return int(is_first_internal and is_second_internal)\n",
    "\n",
    "def select_emails(df, specific_skus):\n",
    "    if specific_skus is None or not specific_skus:\n",
    "        raise ValueError(\"At least one SKU must be provided.\")\n",
    "    if isinstance(specific_skus, str):\n",
    "        specific_skus = [specific_skus]\n",
    "    return df[df['SKU'].isin(specific_skus)]\n",
    "\n",
    "def sample_skus(df, n=30, random_state=1):\n",
    "    \"\"\"\n",
    "    Randomly sample SKUs from a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): DataFrame containing SKUs.\n",
    "    n (int, optional): Number of SKUs to sample. Defaults to 30.\n",
    "    random_state (int, optional): Seed for the random number generator to ensure reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    list: List of sampled SKUs.\n",
    "    \"\"\"\n",
    "    unique_skus = df['SKU'].dropna().unique()\n",
    "    if len(unique_skus) < n:\n",
    "        # If there are fewer unique SKUs than n, return all unique SKUs.\n",
    "        return unique_skus.tolist()\n",
    "    else:\n",
    "        # Randomly sample n SKUs from the unique SKU list.\n",
    "        return df['SKU'].dropna().sample(n, random_state=random_state).tolist()\n",
    "\n",
    "def reorder_id_columns(df):\n",
    "    columns = list(df.columns)\n",
    "    columns.insert(0, columns.pop(columns.index('row_id')))\n",
    "    df = df[columns]\n",
    "    return df\n",
    "\n",
    "def export_emails(df, filepath='sample_amine_emails.csv'):\n",
    "    \"\"\"Export DataFrame to CSV.\"\"\"\n",
    "    df.to_csv(filepath, index=False)\n",
    "    print(f\"CSV file '{filepath}' has been created with the filtered and sorted email data.\")\n",
    "    \n",
    "# Function to extract price from email body\n",
    "def extract_price(body):\n",
    "    # Use regex to find price patterns, assuming prices are formatted as $ followed by numbers\n",
    "    pattern = r\"(?:\\w+\\s){0,30}\\d+[\\d,]*\\s*\\$[\\w\\s]*\"\n",
    "    matches = re.findall(pattern, body)\n",
    "    return matches[0] if matches else None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_csv_filepath = '../Output CSV/Raw Email/amine_emails.csv'\n",
    "prepared_csv_filepath = '../Output CSV/Prepared CSV/test_prepared_amine_emails.csv'\n",
    "price_test_csv_filepath = '../Output CSV/Prepared CSV/price_test_amine_emails.csv'\n",
    "company_csv_filepath = '../Extra Data/amine_unique_emails_company_association.csv'\n",
    "\n",
    "df = load_data(raw_csv_filepath)\n",
    "df_preprocess = preprocess_data(df)\n",
    "df_sku_only_emails = filter_emails_that_contain_sku_in_title(df_preprocess)\n",
    "df_with_company_names = add_company_names(df_sku_only_emails, company_csv_filepath)\n",
    "df_with_all_features = features_creation(df_with_company_names)\n",
    "df_reorder_column = reorder_id_columns(df_with_all_features)\n",
    "\n",
    "# sampling data\n",
    "# list_sample_sku = sample_skus(df_reorder_column, n=30, random_state=1)\n",
    "# df_sample = df_reorder_column[df_reorder_column['SKU'].isin(list_sample_sku)]\n",
    "\n",
    "# test_skus = ['EV-AC-578901']  # Example list of specific SKUs\n",
    "test_skus = [\"EV-AC-578901\", \"EV-JL-558646\", \"EV-NA-459859\", \"EV-NA-511353\", \"EV-NA-518377\"]\n",
    "df_test_sku = select_emails(df_reorder_column, test_skus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sample_with_test_sku = pd.concat([df_sample, df_test_sku]).drop_duplicates()\n",
    "\n",
    "df_test_sku_subset = df_test_sku[['row_id','str_body','SKU','company_connection'\n",
    "                                  ,'email_rank_by_sku_pair'\n",
    "                                  ,'email_rank_by_sku'\n",
    "                                  ,'is_max_rank_equal_to_sku_pair_rank'\n",
    "                                  ,'is_internal_communication'\n",
    "                                  ,'has_no_response'\n",
    "                                  ,'is_not_available']]\n",
    "\n",
    "df_filtered = df_test_sku_subset[df_test_sku_subset['is_max_rank_equal_to_sku_pair_rank'] == True]\n",
    "\n",
    "# add price raw data\n",
    "# need to clean-up for it to work?\n",
    "df_filtered['re_price_info'] = df_filtered['str_body'].apply(extract_price)\n",
    "df_filtered['has_price_info'] = df_filtered['re_price_info'].apply(lambda x: 1 if x is not None else 0)\n",
    "\n",
    "export_emails(df_filtered, filepath=price_test_csv_filepath)\n",
    "#print(df_sample_with_test_sku.shape)\n",
    "#df_sample_with_test_sku.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandasql import sqldf\n",
    "\n",
    "pysqldf = lambda q: sqldf(q, globals())\n",
    "\n",
    "# SQL query for task 1: Count of emails and SKUs by quarter, compute avg emails/SKU\n",
    "query1 = \"\"\"\n",
    "SELECT \n",
    "    date_YYYY_QQ,\n",
    "    COUNT(row_id) AS email_count,\n",
    "    COUNT(DISTINCT SKU) AS unique_skus,\n",
    "    CAST(ROUND(COUNT(row_id) / COUNT(DISTINCT SKU),0) AS INTEGER) AS avg_emails_per_sku\n",
    "FROM \n",
    "    df_reorder_column\n",
    "GROUP BY \n",
    "    date_YYYY_QQ;\n",
    "\"\"\"\n",
    "\n",
    "result1 = pysqldf(query1)\n",
    "result1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    clean_to AS recipient,\n",
    "    company_name_to AS company,\n",
    "    COUNT(row_id) AS emails_sent,\n",
    "    SUM(CASE WHEN date_YYYY_QQ = '2022-Q4' THEN 1 ELSE 0 END) AS Q4_2022,\n",
    "    SUM(CASE WHEN date_YYYY_QQ = '2023-Q1' THEN 1 ELSE 0 END) AS Q1_2023,\n",
    "    SUM(CASE WHEN date_YYYY_QQ = '2023-Q2' THEN 1 ELSE 0 END) AS Q2_2023,\n",
    "    SUM(CASE WHEN date_YYYY_QQ = '2023-Q3' THEN 1 ELSE 0 END) AS Q3_2023,\n",
    "    SUM(CASE WHEN date_YYYY_QQ = '2023-Q4' THEN 1 ELSE 0 END) AS Q4_2023,\n",
    "    SUM(CASE WHEN date_YYYY_QQ = '2024-Q1' THEN 1 ELSE 0 END) AS Q1_2024\n",
    "FROM \n",
    "    df_reorder_column\n",
    "WHERE \n",
    "    clean_from = 'amine.bedaida@bus.com'\n",
    "GROUP BY \n",
    "    clean_to, company\n",
    "ORDER BY \n",
    "    emails_sent DESC;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "result = pysqldf(query)\n",
    "result.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    SKU,\n",
    "    COUNT(*) AS count_emails,\n",
    "    (\n",
    "        SELECT COUNT(DISTINCT company) \n",
    "        FROM (\n",
    "            SELECT company_name_from AS company FROM df_reorder_column sub WHERE sub.SKU = main.SKU AND company_name_from != 'bus.com'\n",
    "            UNION\n",
    "            SELECT company_name_to AS company FROM df_reorder_column sub WHERE sub.SKU = main.SKU AND company_name_to != 'bus.com'\n",
    "        )\n",
    "    ) AS count_unique_companies,\n",
    "    MIN(date_parsed_pd) AS first_email_date,\n",
    "    MAX(date_parsed_pd) AS last_email_date\n",
    "FROM \n",
    "    df_reorder_column main\n",
    "WHERE \n",
    "    SKU IS NOT NULL \n",
    "GROUP BY \n",
    "    SKU\n",
    "ORDER BY \n",
    "    count_emails DESC;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "result = pysqldf(query)\n",
    "\n",
    "result['first_email_date'] = pd.to_datetime(result['first_email_date']).dt.date\n",
    "result['last_email_date'] = pd.to_datetime(result['last_email_date']).dt.date\n",
    "\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Determine the bin edges and midpoints\n",
    "max_value = result['count_unique_companies'].max()\n",
    "bins = range(1, max_value + 2)  # Bins from 1 to max + 1\n",
    "bin_centers = [0.5 * (bins[i] + bins[i+1]) for i in range(len(bins)-1)]  # Calculate midpoints for x-ticks\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(result['count_unique_companies'], bins=bins, kde=False, color='green')\n",
    "plt.title('Distribution Companies Reached Out Per SKU')\n",
    "plt.xlabel('# Companies')\n",
    "plt.ylabel('# SKU')\n",
    "plt.xticks(bin_centers, labels=range(1, max_value + 1))  # Set custom x-ticks at bin centers\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex provide bad result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract price from email body\n",
    "def extract_price(body):\n",
    "    # Use regex to find price patterns, assuming prices are formatted as $ followed by numbers\n",
    "    pattern = r\"(?:\\w+\\s){0,30}\\d+[\\d,]*\\s*\\$[\\w\\s]*\"\n",
    "    matches = re.findall(pattern, body)\n",
    "    return matches[0] if matches else None\n",
    "\n",
    "# Apply the extract_price function to get prices from the email body\n",
    "df_test_sku['price_info'] = df_test_sku['str_body'].apply(extract_price)\n",
    "\n",
    "# now try pivot tables \n",
    "pivot_table = df_test_sku.pivot_table(index='SKU', columns='company_connection'\n",
    "                                            , values='price_info'\n",
    "                                            , aggfunc='first'\n",
    "                                            , fill_value='NA')\n",
    "pivot_table.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anonymize email to leverage Aganta platform for prompt engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anonimized emails dataset\n",
    "test_price_csv_path = '../Test/Data/sample_train_price_amine_emails.csv'\n",
    "df_test_price = pd.read_csv(test_price_csv_path)\n",
    "#email_sample = df_test_price['str_body'].iloc[0]\n",
    "# smaller_email = email_sample[0:100]\n",
    "#print(email_sample)\n",
    "# len(email_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\",\n",
    "                  api_key=\"\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert email anonymizer\"),\n",
    "    (\"human\", (\n",
    "        \"Replace all email addresses, physical adresses, location, company names, website, first names, last names, full names, names, phone numbers, dates (ex: August 4, 2023), job title, and timestamps \"\n",
    "        \"(ex: 10:30am) in this email below (Between \\\"### Start email###\\\" and \\\"###End email###\\\") \"\n",
    "        \"with the following string \\\"[anonymous]\\\"\\n\"\n",
    "        \"### Start email###\\n\"\n",
    "        \"{email_body}\\n\"\n",
    "        \"###End email###\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "#print(chain.invoke({\"email_body\": email_sample}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anonimized_email(email_body):\n",
    "    result = chain.invoke({\"email_body\": email_body})\n",
    "    return result\n",
    "\n",
    "# Apply the function to the 'str_body' column and create a new column with the results\n",
    "df_test_price['ano_email'] = df_test_price['str_body'].apply(anonimized_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the files. Check if anonomized work\n",
    "csv_file_path = '../Test/Data/test_price_amine_ano_emails.csv'\n",
    "df_test_price.to_csv(csv_file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
